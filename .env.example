# LiteLLM Model Configuration
# Format: provider/model_name
# LiteLLM supports 100+ providers with unified interface
#
# Examples:
#   Anthropic: anthropic/claude-3-5-sonnet-20241022
#   OpenAI: openai/gpt-4o
#   Google: gemini/gemini-1.5-pro
#   Azure OpenAI: azure/gpt-4
#   AWS Bedrock: bedrock/anthropic.claude-v2
#   Local (Ollama): ollama/llama2
LITELLM_MODEL=anthropic/claude-3-5-sonnet-20241022

# API Keys
# Set the key for your chosen provider (LiteLLM auto-detects based on model prefix)
ANTHROPIC_API_KEY=your_anthropic_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here

# Additional Provider API Keys (optional, only if you use these providers)
AZURE_API_KEY=your_azure_api_key_here
COHERE_API_KEY=your_cohere_api_key_here
HUGGINGFACE_API_KEY=your_huggingface_api_key_here
REPLICATE_API_KEY=your_replicate_api_key_here
TOGETHER_API_KEY=your_together_api_key_here

# Optional: Custom base URL for proxies or custom endpoints
# Leave empty to use the default API endpoints
LITELLM_API_BASE=

# Optional: LiteLLM-specific settings
LITELLM_DROP_PARAMS=true       # Drop unsupported params instead of erroring
LITELLM_TIMEOUT=600            # Request timeout in seconds

# Tool Configuration
TOOL_TIMEOUT=600               # Default tool timeout in seconds (per-call override via tool input)

# Agent Configuration
MAX_ITERATIONS=100

# Retry Configuration (for handling rate limits and API errors)
# Note: Reduced from 5 to 3 as LiteLLM has built-in retry logic
RETRY_MAX_ATTEMPTS=3           # Maximum number of retry attempts
RETRY_INITIAL_DELAY=1.0        # Initial delay in seconds
RETRY_MAX_DELAY=60.0           # Maximum delay in seconds

# Memory Management Configuration
MEMORY_ENABLED=true                   # Enable/disable memory compression
MEMORY_COMPRESSION_THRESHOLD=25000     # Token threshold - compress when exceeded
MEMORY_SHORT_TERM_SIZE=100             # Number of recent messages to keep
MEMORY_COMPRESSION_RATIO=0.3           # Target compression ratio (0.3 = 30% of original)

# Logging Configuration
LOG_DIR=logs                           # Directory for log files
LOG_LEVEL=DEBUG                        # Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_TO_FILE=true                       # Enable/disable file logging
LOG_TO_CONSOLE=false                   # Enable/disable console logging (WARNING and above)

# ============================================================================
# LiteLLM Provider Examples
# ============================================================================
#
# Anthropic Claude:
#   LITELLM_MODEL=anthropic/claude-3-5-sonnet-20241022
#   LITELLM_MODEL=anthropic/claude-3-opus-20240229
#   LITELLM_MODEL=anthropic/claude-3-haiku-20240307
#
# OpenAI:
#   LITELLM_MODEL=openai/gpt-4o
#   LITELLM_MODEL=openai/gpt-4-turbo
#   LITELLM_MODEL=openai/gpt-3.5-turbo
#
# Google Gemini:
#   LITELLM_MODEL=gemini/gemini-1.5-pro
#   LITELLM_MODEL=gemini/gemini-1.5-flash
#   LITELLM_MODEL=gemini/gemini-2.0-flash-exp
#
# Azure OpenAI (requires AZURE_API_BASE and AZURE_API_VERSION):
#   LITELLM_MODEL=azure/gpt-4
#   AZURE_API_BASE=https://your-resource.openai.azure.com
#   AZURE_API_VERSION=2024-02-15-preview
#
# AWS Bedrock:
#   LITELLM_MODEL=bedrock/anthropic.claude-v2
#   LITELLM_MODEL=bedrock/anthropic.claude-3-sonnet-20240229-v1:0
#
# Cohere:
#   LITELLM_MODEL=cohere/command-r-plus
#   LITELLM_MODEL=cohere/command-r
#
# Local Models (Ollama):
#   LITELLM_MODEL=ollama/llama2
#   LITELLM_MODEL=ollama/mistral
#   LITELLM_MODEL=ollama/codellama
#
# HuggingFace:
#   LITELLM_MODEL=huggingface/meta-llama/Llama-2-7b-chat-hf
#
# Together AI:
#   LITELLM_MODEL=together_ai/togethercomputer/llama-2-70b-chat
#
# For more providers and models, see: https://docs.litellm.ai/docs/providers
