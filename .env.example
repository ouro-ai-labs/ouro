# LLM Provider (anthropic, openai, or gemini)
LLM_PROVIDER=gemini

# API Keys (set the one for your chosen provider)
ANTHROPIC_API_KEY=your_anthropic_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here

# Model (optional, defaults to provider-specific models)
# Anthropic: claude-3-5-sonnet-20241022, claude-3-opus-20240229, etc.
# OpenAI: gpt-4o, gpt-4-turbo, gpt-3.5-turbo, etc.
# Gemini: gemini-1.5-pro, gemini-1.5-flash, etc.
MODEL=gemini-2.5-flash

# Base URL (optional, for custom endpoints, proxies, or local deployments)
# Leave empty to use the default API endpoints
ANTHROPIC_BASE_URL=                    # Example: https://api.xiaomimimo.com/anthropic
OPENAI_BASE_URL=                       # Example: https://api.openai-proxy.com/v1
GEMINI_BASE_URL=                       # Note: Gemini doesn't support custom base_url

# Agent Configuration
MAX_ITERATIONS=10

# Tool Configuration
ENABLE_SHELL=true

# Retry Configuration (for handling rate limits and API errors)
RETRY_MAX_ATTEMPTS=5           # Maximum number of retry attempts
RETRY_INITIAL_DELAY=1.0        # Initial delay in seconds
RETRY_MAX_DELAY=60.0           # Maximum delay in seconds

# Memory Management Configuration
MEMORY_ENABLED=true                    # Enable/disable memory compression
MEMORY_MAX_CONTEXT_TOKENS=100000       # Maximum context window size
MEMORY_TARGET_TOKENS=50000             # Target working memory size (soft limit)
MEMORY_COMPRESSION_THRESHOLD=40000     # Hard limit - compress when exceeded
MEMORY_SHORT_TERM_SIZE=20              # Number of recent messages to keep
MEMORY_COMPRESSION_RATIO=0.3           # Target compression ratio (0.3 = 30% of original)
